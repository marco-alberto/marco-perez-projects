{
  
    
        "post0": {
            "title": "COVID_19_Detection",
            "content": ". from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import random, os, shutil import copy import imageio import PIL import numpy as np import cv2 . from zipfile import ZipFile datasets = [&#39;Train&#39;, &#39;Validation&#39;] for dataset in datasets: with ZipFile(&#39;/content/drive/My Drive/COVID-19_Dataset/{}.zip&#39;.format(dataset), &#39;r&#39;) as zip: #&#39;/content/drive/My Drive/COVID-19_Dataset/Train.zip&#39; zip.extractall(&#39;COVID-19_Dataset&#39;) . Referenciamos los conjuntos de entrenamiento y validaci&#243;n . train_dir = os.path.join(&#39;COVID-19_Dataset/Train&#39;) validation_dir = os.path.join(&#39;COVID-19_Dataset/Validation&#39;) . Sistema de detecci&#243;n # 1 . Definici&#243;n de la arquitectura . from tensorflow.keras import layers from tensorflow.keras import models #layers.Conv2D -&gt; Capa convolucional #layers.MaxPooling2D -&gt; Capa max pooling model = models.Sequential() model.add(layers.Conv2D(32,kernel_size=(3,3),activation=&#39;relu&#39;,input_shape=(224,224,3))) model.add(layers.Conv2D(128,(3,3),activation=&#39;relu&#39;)) model.add(layers.MaxPooling2D(pool_size=(2,2))) model.add(layers.Dropout(0.25)) #Regularizar el rendimiento de su CNN o mejora el desempeño de su CNN model.add(layers.Conv2D(64,(3,3),activation=&#39;relu&#39;)) model.add(layers.MaxPooling2D(pool_size=(2,2))) model.add(layers.Dropout(0.25)) model.add(layers.Conv2D(128,(3,3),activation=&#39;relu&#39;)) model.add(layers.MaxPooling2D(pool_size=(2,2))) model.add(layers.Dropout(0.25)) model.add(layers.Flatten()) model.add(layers.Dense(64,activation=&#39;relu&#39;)) model.add(layers.Dropout(0.5)) model.add(layers.Dense(1,activation=&#39;sigmoid&#39;)) model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 222, 222, 32) 896 _________________________________________________________________ conv2d_1 (Conv2D) (None, 220, 220, 128) 36992 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 110, 110, 128) 0 _________________________________________________________________ dropout (Dropout) (None, 110, 110, 128) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 108, 108, 64) 73792 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 54, 54, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 54, 54, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 52, 52, 128) 73856 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 26, 26, 128) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 26, 26, 128) 0 _________________________________________________________________ flatten (Flatten) (None, 86528) 0 _________________________________________________________________ dense (Dense) (None, 64) 5537856 _________________________________________________________________ dropout_3 (Dropout) (None, 64) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 65 ================================================================= Total params: 5,723,457 Trainable params: 5,723,457 Non-trainable params: 0 _________________________________________________________________ . Compilaci&#243;n . from tensorflow.keras import optimizers model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, #SGD metrics=[&#39;acc&#39;, &#39;Precision&#39;, &#39;Recall&#39;]) . Definici&#243;n de los generadores . Un generador te permite manipular las imágenes de tu base de datos . from tensorflow.keras.preprocessing.image import ImageDataGenerator #Conjunto de entrenamiento train_datagen = ImageDataGenerator(rescale=1./255, #Normalización shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True) #Conjunto de validación test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_directory( train_dir, # Target directory target_size=(224, 224), # All images are resized from 240x320 to 72x96 batch_size= 32, color_mode=&#39;rgb&#39;, class_mode=&#39;binary&#39;, shuffle=True) validation_generator = test_datagen.flow_from_directory( validation_dir, target_size=(224, 224), batch_size=1, color_mode=&#39;rgb&#39;, class_mode=&#39;binary&#39;, shuffle=False) . Found 224 images belonging to 2 classes. Found 60 images belonging to 2 classes. . Entrenamiento . from tensorflow.keras.callbacks import ModelCheckpoint mc = ModelCheckpoint(&#39;/content/drive/My Drive/COVID-19_Dataset/covid_detection.h5&#39;, monitor=&#39;val_acc&#39;, mode=&#39;max&#39;, verbose=1, save_best_only=True) . history = model.fit( train_generator, steps_per_epoch=7, #70 epochs= 10, validation_data=validation_generator, validation_steps=60, callbacks = [mc]) . Epoch 1/10 2/7 [=======&gt;......................] - ETA: 0s - loss: 2.1142 - acc: 0.5312 - precision: 0.5577 - recall: 0.8056WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0698s vs `on_train_batch_end` time: 0.1261s). Check your callbacks. 7/7 [==============================] - ETA: 0s - loss: 1.6038 - acc: 0.4866 - precision: 0.4878 - recall: 0.5357 Epoch 00001: val_acc improved from -inf to 0.50000, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection.h5 7/7 [==============================] - 7s 980ms/step - loss: 1.6038 - acc: 0.4866 - precision: 0.4878 - recall: 0.5357 - val_loss: 0.6904 - val_acc: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 Epoch 2/10 7/7 [==============================] - ETA: 0s - loss: 0.6713 - acc: 0.5893 - precision: 0.5893 - recall: 0.5893 Epoch 00002: val_acc improved from 0.50000 to 0.51667, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection.h5 7/7 [==============================] - 7s 941ms/step - loss: 0.6713 - acc: 0.5893 - precision: 0.5893 - recall: 0.5893 - val_loss: 0.6673 - val_acc: 0.5167 - val_precision: 0.5085 - val_recall: 1.0000 Epoch 3/10 7/7 [==============================] - ETA: 0s - loss: 0.5123 - acc: 0.7991 - precision: 0.7913 - recall: 0.8125 Epoch 00003: val_acc improved from 0.51667 to 0.95000, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection.h5 7/7 [==============================] - 7s 939ms/step - loss: 0.5123 - acc: 0.7991 - precision: 0.7913 - recall: 0.8125 - val_loss: 0.3811 - val_acc: 0.9500 - val_precision: 0.9655 - val_recall: 0.9333 Epoch 4/10 7/7 [==============================] - ETA: 0s - loss: 0.3560 - acc: 0.8527 - precision: 0.8624 - recall: 0.8393 Epoch 00004: val_acc improved from 0.95000 to 0.96667, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection.h5 7/7 [==============================] - 7s 947ms/step - loss: 0.3560 - acc: 0.8527 - precision: 0.8624 - recall: 0.8393 - val_loss: 0.2667 - val_acc: 0.9667 - val_precision: 0.9667 - val_recall: 0.9667 Epoch 5/10 7/7 [==============================] - ETA: 0s - loss: 0.3039 - acc: 0.9107 - precision: 0.9035 - recall: 0.9196 Epoch 00005: val_acc did not improve from 0.96667 7/7 [==============================] - 6s 925ms/step - loss: 0.3039 - acc: 0.9107 - precision: 0.9035 - recall: 0.9196 - val_loss: 0.1520 - val_acc: 0.9667 - val_precision: 0.9667 - val_recall: 0.9667 Epoch 6/10 7/7 [==============================] - ETA: 0s - loss: 0.2389 - acc: 0.9107 - precision: 0.9182 - recall: 0.9018 Epoch 00006: val_acc improved from 0.96667 to 0.98333, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection.h5 7/7 [==============================] - 7s 1s/step - loss: 0.2389 - acc: 0.9107 - precision: 0.9182 - recall: 0.9018 - val_loss: 0.1327 - val_acc: 0.9833 - val_precision: 1.0000 - val_recall: 0.9667 Epoch 7/10 7/7 [==============================] - ETA: 0s - loss: 0.1259 - acc: 0.9643 - precision: 0.9561 - recall: 0.9732 Epoch 00007: val_acc did not improve from 0.98333 7/7 [==============================] - 7s 933ms/step - loss: 0.1259 - acc: 0.9643 - precision: 0.9561 - recall: 0.9732 - val_loss: 0.0684 - val_acc: 0.9833 - val_precision: 1.0000 - val_recall: 0.9667 Epoch 8/10 7/7 [==============================] - ETA: 0s - loss: 0.2013 - acc: 0.9375 - precision: 0.9537 - recall: 0.9196 Epoch 00008: val_acc did not improve from 0.98333 7/7 [==============================] - 6s 916ms/step - loss: 0.2013 - acc: 0.9375 - precision: 0.9537 - recall: 0.9196 - val_loss: 0.0899 - val_acc: 0.9667 - val_precision: 1.0000 - val_recall: 0.9333 Epoch 9/10 7/7 [==============================] - ETA: 0s - loss: 0.2577 - acc: 0.9196 - precision: 0.8917 - recall: 0.9554 Epoch 00009: val_acc did not improve from 0.98333 7/7 [==============================] - 6s 898ms/step - loss: 0.2577 - acc: 0.9196 - precision: 0.8917 - recall: 0.9554 - val_loss: 0.2508 - val_acc: 0.9167 - val_precision: 1.0000 - val_recall: 0.8333 Epoch 10/10 7/7 [==============================] - ETA: 0s - loss: 0.2568 - acc: 0.9241 - precision: 0.9524 - recall: 0.8929 Epoch 00010: val_acc did not improve from 0.98333 7/7 [==============================] - 6s 916ms/step - loss: 0.2568 - acc: 0.9241 - precision: 0.9524 - recall: 0.8929 - val_loss: 0.2022 - val_acc: 0.9833 - val_precision: 1.0000 - val_recall: 0.9667 . Curvas de precisi&#243;n y p&#233;rdida . import matplotlib.pyplot as plt acc = history.history[&#39;acc&#39;] val_acc = history.history[&#39;val_acc&#39;] max_val_acc = max(val_acc) max_val_acc_epoch= val_acc.index(max(val_acc)) + 1 loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] min_val_loss = min(val_loss) min_val_loss_epoch= val_loss.index(min(val_loss)) + 1 epochs = range(len(acc)) plt.plot(epochs, acc, &#39;go&#39;, label=&#39;Precisión en el entrenamiento&#39;) plt.plot(epochs, val_acc, &#39;r&#39;, label=&#39;Precisión en la validación&#39;) plt.plot(max_val_acc_epoch, max_val_acc, &#39;bo&#39;, label=&#39;Maximum accuracy&#39;) plt.title(&#39;Precisión durante el entrenamiento y la validación&#39;) plt.legend() print(&quot;Best accuracy epoch : % d, Value : % .5f&quot; %(max_val_acc_epoch, max_val_acc)) plt.figure() plt.plot(epochs, loss, &#39;go&#39;, label=&#39;Pérdida en el entrenamiento&#39;) plt.plot(epochs, val_loss, &#39;r&#39;, label=&#39;Pérdida en la validación&#39;) plt.plot(min_val_loss_epoch, min_val_loss, &#39;bo&#39;, label=&#39;Minimum loss&#39;) plt.title(&#39;Pérdida durante el entrenamiento y la validación&#39;) plt.legend() print(&quot;Best loss epoch : % d, Value : % .10f&quot; %(min_val_loss_epoch, min_val_loss)) plt.show() . Best accuracy epoch : 6, Value : 0.98333 Best loss epoch : 7, Value : 0.0683839396 . Evaluaci&#243;n del desempe&#241;o mediante matriz de confusi&#243;n y F1 . import tensorflow model = tensorflow.keras.models.load_model(&#39;/content/drive/My Drive/COVID-19_Dataset/covid_detection.h5&#39;, compile=False) . Matriz de confusión . from sklearn.metrics import confusion_matrix test_predictions = model.predict_generator(validation_generator, 60) # model.predict_generator(generador, num_lotes_por_epoca) . WARNING:tensorflow:From &lt;ipython-input-7-5adcdd27d6a4&gt;:3: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version. Instructions for updating: Please use Model.predict, which supports generators. . test_predictions . array([[0.33285046], [0.03652506], [0.3113044 ], [0.01680768], [0.0280006 ], [0.03571345], [0.04017631], [0.12114099], [0.00770472], [0.0190876 ], [0.31307364], [0.02349256], [0.13990952], [0.10045203], [0.04120576], [0.2399632 ], [0.01806355], [0.18598503], [0.02390642], [0.06023042], [0.09811092], [0.25975892], [0.02162566], [0.08230507], [0.02493787], [0.00604276], [0.15060468], [0.18285762], [0.05528783], [0.27212125], [0.89742553], [0.94193673], [0.887836 ], [0.56038946], [0.9136642 ], [0.92812467], [0.93489206], [0.9523687 ], [0.94278693], [0.9112616 ], [0.9394306 ], [0.9225314 ], [0.932964 ], [0.8832737 ], [0.9114831 ], [0.9096755 ], [0.9288597 ], [0.9457074 ], [0.9057664 ], [0.84018546], [0.87893605], [0.8439931 ], [0.93442655], [0.946909 ], [0.9374663 ], [0.92456806], [0.9592042 ], [0.9295186 ], [0.8773518 ], [0.28127992]], dtype=float32) . #y que las predicciones con valores menores a 0.5, sean clasificadas como Negativas (False) test_predictions = (test_predictions &gt; 0.5) print(test_predictions) . [[False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [False] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [ True] [False]] . validation_generator.classes . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32) . from sklearn.metrics import confusion_matrix cm = confusion_matrix(validation_generator.classes, test_predictions) print(&#39;Confusion Matrix&#39;) print(cm) . Confusion Matrix [[30 0] [ 1 29]] . import matplotlib.pyplot as plt import seaborn as sns fig, ax = plt.subplots(figsize=(5, 5)) ax.matshow(cm, cmap=plt.cm.cividis_r, alpha=0.5) for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(x=j, y=i, s=cm[i, j], va=&#39;center&#39;, ha=&#39;center&#39;) plt.title(&quot;Matriz de confusión&quot;, fontsize=15) plt.xlabel(&#39;Predicciones&#39;) plt.ylabel(&#39;Valores verdaderos o etiquetas&#39;) plt.tight_layout() plt.show() . Calculamos la métrica F1 . from sklearn.metrics import precision_score from sklearn.metrics import recall_score, f1_score print(&#39;Precision: %.7f&#39; % precision_score(y_true=validation_generator.classes, y_pred=test_predictions)) print(&#39;Recall: %.7f&#39; % recall_score(y_true=validation_generator.classes, y_pred=test_predictions)) print(&#39;F1: %.7f&#39; % f1_score(y_true=validation_generator.classes, y_pred=test_predictions)) . Precision: 1.0000000 Recall: 0.9666667 F1: 0.9830508 . Sistema de detecci&#243;n # 2 (Basado en transfer learning) . Carga de la arquitectura VGG16 . from tensorflow.keras.applications import VGG16 conv_base = VGG16(weights=&#39;imagenet&#39;, include_top=False, input_shape=(224, 224, 3)) conv_base.summary() . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 58892288/58889256 [==============================] - 1s 0us/step Model: &#34;vgg16&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 ================================================================= Total params: 14,714,688 Trainable params: 14,714,688 Non-trainable params: 0 _________________________________________________________________ . Congelamos y descongelamos ciertas capas (Fine-tuning) . for layer in conv_base.layers: if layer.name[:6] == &#39;block5&#39;: layer.trainable = True else: layer.trainable = False conv_base.summary() . Model: &#34;vgg16&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 ================================================================= Total params: 14,714,688 Trainable params: 7,079,424 Non-trainable params: 7,635,264 _________________________________________________________________ . Definici&#243;n de la arquitectura (incluyendo el m&#243;dulo de transfer learning) . from tensorflow.keras import layers from tensorflow.keras import models model = models.Sequential() model.add(conv_base) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.BatchNormalization()) model.add(layers.Flatten()) model.add(layers.Dense(1, activation=&#39;sigmoid&#39;)) model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= vgg16 (Functional) (None, 7, 7, 512) 14714688 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 3, 3, 512) 0 _________________________________________________________________ batch_normalization (BatchNo (None, 3, 3, 512) 2048 _________________________________________________________________ flatten (Flatten) (None, 4608) 0 _________________________________________________________________ dense (Dense) (None, 1) 4609 ================================================================= Total params: 14,721,345 Trainable params: 7,085,057 Non-trainable params: 7,636,288 _________________________________________________________________ . Compilaci&#243;n . from tensorflow.keras import optimizers model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=optimizers.RMSprop(lr=1e-4), metrics=[&#39;acc&#39;, &#39;Precision&#39;, &#39;Recall&#39;]) . Entrenamiento . from tensorflow.keras.callbacks import ModelCheckpoint mc = ModelCheckpoint(&#39;/content/drive/My Drive/COVID-19_Dataset/covid_detection_vgg16.h5&#39;, monitor=&#39;val_acc&#39;, mode=&#39;max&#39;, verbose=1, save_best_only=True) . history = model.fit( train_generator, steps_per_epoch=7, #70 epochs= 10, validation_data=validation_generator, validation_steps=60, callbacks = [mc]) . Epoch 1/10 7/7 [==============================] - ETA: 0s - loss: 0.4044 - acc: 0.8348 - precision: 0.8205 - recall: 0.8571 Epoch 00001: val_acc improved from -inf to 0.50000, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection_vgg16.h5 7/7 [==============================] - 7s 1s/step - loss: 0.4044 - acc: 0.8348 - precision: 0.8205 - recall: 0.8571 - val_loss: 0.7113 - val_acc: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 Epoch 2/10 7/7 [==============================] - ETA: 0s - loss: 0.0158 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 Epoch 00002: val_acc improved from 0.50000 to 0.60000, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection_vgg16.h5 7/7 [==============================] - 8s 1s/step - loss: 0.0158 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.5254 - val_acc: 0.6000 - val_precision: 0.5556 - val_recall: 1.0000 Epoch 3/10 7/7 [==============================] - ETA: 0s - loss: 0.0036 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 Epoch 00003: val_acc improved from 0.60000 to 0.68333, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection_vgg16.h5 7/7 [==============================] - 8s 1s/step - loss: 0.0036 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.5029 - val_acc: 0.6833 - val_precision: 0.6122 - val_recall: 1.0000 Epoch 4/10 7/7 [==============================] - ETA: 0s - loss: 0.0046 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 Epoch 00004: val_acc improved from 0.68333 to 0.76667, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection_vgg16.h5 7/7 [==============================] - 7s 1s/step - loss: 0.0046 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.4468 - val_acc: 0.7667 - val_precision: 0.6818 - val_recall: 1.0000 Epoch 5/10 7/7 [==============================] - ETA: 0s - loss: 0.0020 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 Epoch 00005: val_acc improved from 0.76667 to 0.85000, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection_vgg16.h5 7/7 [==============================] - 8s 1s/step - loss: 0.0020 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.3046 - val_acc: 0.8500 - val_precision: 0.7692 - val_recall: 1.0000 Epoch 6/10 7/7 [==============================] - ETA: 0s - loss: 0.0030 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 Epoch 00006: val_acc did not improve from 0.85000 7/7 [==============================] - 7s 1s/step - loss: 0.0030 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.3034 - val_acc: 0.8500 - val_precision: 0.7692 - val_recall: 1.0000 Epoch 7/10 7/7 [==============================] - ETA: 0s - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 Epoch 00007: val_acc improved from 0.85000 to 0.86667, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection_vgg16.h5 7/7 [==============================] - 8s 1s/step - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.2860 - val_acc: 0.8667 - val_precision: 0.7895 - val_recall: 1.0000 Epoch 8/10 7/7 [==============================] - ETA: 0s - loss: 8.1647e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 Epoch 00008: val_acc improved from 0.86667 to 0.88333, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection_vgg16.h5 7/7 [==============================] - 7s 1s/step - loss: 8.1647e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.2477 - val_acc: 0.8833 - val_precision: 0.8108 - val_recall: 1.0000 Epoch 9/10 7/7 [==============================] - ETA: 0s - loss: 3.3012e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 Epoch 00009: val_acc improved from 0.88333 to 0.91667, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection_vgg16.h5 7/7 [==============================] - 8s 1s/step - loss: 3.3012e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.2043 - val_acc: 0.9167 - val_precision: 0.8571 - val_recall: 1.0000 Epoch 10/10 7/7 [==============================] - ETA: 0s - loss: 3.6751e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 Epoch 00010: val_acc improved from 0.91667 to 0.96667, saving model to /content/drive/My Drive/COVID-19_Dataset/covid_detection_vgg16.h5 7/7 [==============================] - 7s 1s/step - loss: 3.6751e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.1370 - val_acc: 0.9667 - val_precision: 0.9375 - val_recall: 1.0000 . Curvas de precisi&#243;n y p&#233;rdida . import matplotlib.pyplot as plt acc = history.history[&#39;acc&#39;] val_acc = history.history[&#39;val_acc&#39;] max_val_acc = max(val_acc) max_val_acc_epoch= val_acc.index(max(val_acc)) + 1 loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] min_val_loss = min(val_loss) min_val_loss_epoch= val_loss.index(min(val_loss)) + 1 epochs = range(len(acc)) plt.plot(epochs, acc, &#39;go&#39;, label=&#39;Precisión en el entrenamiento&#39;) plt.plot(epochs, val_acc, &#39;r&#39;, label=&#39;Precisión en la validación&#39;) plt.plot(max_val_acc_epoch, max_val_acc, &#39;bo&#39;, label=&#39;Maximum accuracy&#39;) plt.title(&#39;Precisión durante el entrenamiento y la validación&#39;) plt.legend() print(&quot;Best accuracy epoch : % d, Value : % .5f&quot; %(max_val_acc_epoch, max_val_acc)) plt.figure() plt.plot(epochs, loss, &#39;go&#39;, label=&#39;Pérdida en el entrenamiento&#39;) plt.plot(epochs, val_loss, &#39;r&#39;, label=&#39;Pérdida en la validación&#39;) plt.plot(min_val_loss_epoch, min_val_loss, &#39;bo&#39;, label=&#39;Minimum loss&#39;) plt.title(&#39;Pérdida durante el entrenamiento y la validación&#39;) plt.legend() print(&quot;Best loss epoch : % d, Value : % .10f&quot; %(min_val_loss_epoch, min_val_loss)) plt.show() . Best accuracy epoch : 10, Value : 0.96667 Best loss epoch : 10, Value : 0.1370492429 . Evaluaci&#243;n del desempe&#241;o mediante matriz de confusi&#243;n y F1 . model = tensorflow.keras.models.load_model(&#39;/content/drive/My Drive/COVID-19_Dataset/covid_detection_vgg16.h5&#39;, compile=False) . Matriz de confusión . test_predictions = model.predict_generator(validation_generator, 60) test_predictions = (test_predictions &gt; 0.5) cm = confusion_matrix(validation_generator.classes, test_predictions) print(&#39;Confusion Matrix&#39;) print(cm) . Confusion Matrix [[28 2] [ 0 30]] . import matplotlib.pyplot as plt import seaborn as sns fig, ax = plt.subplots(figsize=(5, 5)) ax.matshow(cm, cmap=plt.cm.ocean, alpha=0.5) for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(x=j, y=i, s=cm[i, j], va=&#39;center&#39;, ha=&#39;center&#39;) plt.title(&quot;Matriz de confusión&quot;, fontsize=15) plt.xlabel(&#39;Predicciones&#39;) plt.ylabel(&#39;Valores verdaderos o etiquetas&#39;) plt.tight_layout() plt.show() . Calculamos la métrica F1 . from sklearn.metrics import precision_score from sklearn.metrics import recall_score, f1_score print(&#39;Precision: %.7f&#39; % precision_score(y_true=validation_generator.classes, y_pred=test_predictions)) print(&#39;Recall: %.7f&#39; % recall_score(y_true=validation_generator.classes, y_pred=test_predictions)) print(&#39;F1: %.7f&#39; % f1_score(y_true=validation_generator.classes, y_pred=test_predictions)) . Precision: 0.9375000 Recall: 1.0000000 F1: 0.9677419 .",
            "url": "https://marco-alberto.github.io/marco-perez-projects/2022/04/21/_04_22_COVID_19_Detection.html",
            "relUrl": "/2022/04/21/_04_22_COVID_19_Detection.html",
            "date": " • Apr 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Cats And Dogs",
            "content": ". from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . La base de datos cats vs. dogs, se encuentra en el sitio https://www.kaggle.com/c/dogs-vs-cats/data y fue publicada por el sitio https://www.kaggle.com/ en el 2013 como parte de una competencia de visión computacional. Contiene un total de 25,000 imágenes, 12,500 de gatos y 12,500 de perros. En nuestro caso, usaremos un subconjunto de dicha base de datos. . import os, shutil import zipfile #Extracción de la base de datos local_zip = &#39;/content/drive/My Drive/CNNs/cats_and_dogs_filtered.zip&#39; zip_ref = zipfile.ZipFile(local_zip, &#39;r&#39;) zip_ref.extractall(&#39;/content/&#39;) zip_ref.close() . import os # Carpeta donde se almacena la base de datos descargada base_dir = &#39;/content/cats_and_dogs_filtered&#39; # Carpetas donde se almacenan los conjuntos de entrenamiento y validación train_dir = os.path.join(base_dir, &#39;train&#39;) # &#39;/content/drive/My Drive/CNNs/cats_and_dogs_filtered/train&#39; validation_dir = os.path.join(base_dir, &#39;validation&#39;) # &#39;/content/drive/My Drive/CNNs/cats_and_dogs_filtered/validation&#39; # Carpeta donde se almacenan las imágenes de gatos del conjunto de entrenamiento train_cats_dir = os.path.join(train_dir, &#39;cats&#39;) #&#39;/content/drive/My Drive/CNNs/cats_and_dogs_filtered/train/cats&#39; # Carpeta donde se almacenan las imágenes de perros del conjunto de entrenamiento train_dogs_dir = os.path.join(train_dir, &#39;dogs&#39;) #&#39;/content/drive/My Drive/CNNs/cats_and_dogs_filtered/train/dogs&#39; # Carpeta donde se almacenan las imágenes de gatos del conjunto de validación validation_cats_dir = os.path.join(validation_dir, &#39;cats&#39;) # &#39;/content/drive/My Drive/CNNs/cats_and_dogs_filtered/validation/cats&#39; # Carpeta donde se almacenan las imágenes de perros del conjunto de validación validation_dogs_dir = os.path.join(validation_dir, &#39;dogs&#39;) # # &#39;/content/drive/My Drive/CNNs/cats_and_dogs_filtered/validation/dogs&#39; . print(&#39;Número de imágenes de gatos pertencientes al conjunto de entrenamiento:&#39;, len(os.listdir(train_cats_dir))) . Número de imágenes de gatos pertencientes al conjunto de entrenamiento: 1000 . print(&#39;Número de imágenes de perros pertencientes al conjunto de entrenamiento:&#39;, len(os.listdir(train_dogs_dir))) . Número de imágenes de perros pertencientes al conjunto de entrenamiento: 1000 . print(&#39;Número de imágenes de gatos pertencientes al conjunto de validación:&#39;, len(os.listdir(validation_cats_dir))) . Número de imágenes de gatos pertencientes al conjunto de validación: 500 . print(&#39;Número de imágenes de perros pertencientes al conjunto de validación::&#39;, len(os.listdir(validation_dogs_dir))) . Número de imágenes de perros pertencientes al conjunto de validación:: 500 . Construcci&#243;n de la CNN . from keras import layers from keras import models model = models.Sequential() model.add(layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(150, 150, 3))) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;)) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(128, (3, 3), activation=&#39;relu&#39;)) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(128, (3, 3), activation=&#39;relu&#39;)) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Flatten()) model.add(layers.Dense(512, activation=&#39;relu&#39;)) model.add(layers.Dense(1, activation=&#39;sigmoid&#39;)) . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 148, 148, 32) 896 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 74, 74, 32) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 72, 72, 64) 18496 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 34, 34, 128) 73856 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 15, 15, 128) 147584 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128) 0 _________________________________________________________________ flatten (Flatten) (None, 6272) 0 _________________________________________________________________ dense (Dense) (None, 512) 3211776 _________________________________________________________________ dense_1 (Dense) (None, 1) 513 ================================================================= Total params: 3,453,121 Trainable params: 3,453,121 Non-trainable params: 0 _________________________________________________________________ . Compilaci&#243;n . from keras import optimizers model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=optimizers.RMSprop(lr=1e-4), metrics=[&#39;acc&#39;]) # acc -&gt; accuracy . Preprocesamiento de las im&#225;genes de entrada . from keras.preprocessing.image import ImageDataGenerator #Sirve para acceder y manipular las instancias de la base de datos #Todas las imágenes se normalizan en el rango de [0,1] train_datagen = ImageDataGenerator(rescale=1./255) test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_directory( # Carpeta del conjunto de entrenamiento train_dir, # Las imágenes se redimensionan a 150 X 150 target_size=(150, 150), batch_size=20, #Mediante el argumento class_mode, se define el tipo de clasificación que se llevará a cabo. En este caso, es clasificación binaria. class_mode=&#39;binary&#39;) validation_generator = test_datagen.flow_from_directory( validation_dir, target_size=(150, 150), batch_size=20, class_mode=&#39;binary&#39;) . Found 2000 images belonging to 2 classes. Found 1000 images belonging to 2 classes. . for data_batch, labels_batch in train_generator: print(&#39;data batch shape:&#39;, data_batch.shape) print(&#39;labels batch shape:&#39;, labels_batch.shape) break . data batch shape: (20, 150, 150, 3) labels batch shape: (20,) . Entrenamiento . history = model.fit( train_generator, #Conjunto de entrenamiento steps_per_epoch=100, #Número de veces que se tomarán 20 (batch_size) lotes de imágenes epochs=30, validation_data=validation_generator, validation_steps=50) . Epoch 1/30 100/100 [==============================] - 9s 87ms/step - loss: 0.6843 - acc: 0.5455 - val_loss: 0.6571 - val_acc: 0.6140 Epoch 2/30 100/100 [==============================] - 9s 86ms/step - loss: 0.6317 - acc: 0.6405 - val_loss: 0.6179 - val_acc: 0.6320 Epoch 3/30 100/100 [==============================] - 9s 86ms/step - loss: 0.5805 - acc: 0.6905 - val_loss: 0.6280 - val_acc: 0.6380 Epoch 4/30 100/100 [==============================] - 9s 86ms/step - loss: 0.5415 - acc: 0.7160 - val_loss: 0.5628 - val_acc: 0.7050 Epoch 5/30 100/100 [==============================] - 8s 85ms/step - loss: 0.5193 - acc: 0.7345 - val_loss: 0.5414 - val_acc: 0.7260 Epoch 6/30 100/100 [==============================] - 8s 85ms/step - loss: 0.4807 - acc: 0.7710 - val_loss: 0.5462 - val_acc: 0.7170 Epoch 7/30 100/100 [==============================] - 9s 85ms/step - loss: 0.4613 - acc: 0.7845 - val_loss: 0.5389 - val_acc: 0.7280 Epoch 8/30 100/100 [==============================] - 9s 86ms/step - loss: 0.4385 - acc: 0.7930 - val_loss: 0.5231 - val_acc: 0.7460 Epoch 9/30 100/100 [==============================] - 9s 86ms/step - loss: 0.4073 - acc: 0.8150 - val_loss: 0.5651 - val_acc: 0.7140 Epoch 10/30 100/100 [==============================] - 9s 86ms/step - loss: 0.3848 - acc: 0.8275 - val_loss: 0.5930 - val_acc: 0.7160 Epoch 11/30 100/100 [==============================] - 9s 86ms/step - loss: 0.3619 - acc: 0.8410 - val_loss: 0.5451 - val_acc: 0.7440 Epoch 12/30 100/100 [==============================] - 8s 85ms/step - loss: 0.3505 - acc: 0.8510 - val_loss: 0.5269 - val_acc: 0.7510 Epoch 13/30 100/100 [==============================] - 8s 84ms/step - loss: 0.3238 - acc: 0.8715 - val_loss: 0.5593 - val_acc: 0.7480 Epoch 14/30 100/100 [==============================] - 8s 84ms/step - loss: 0.3089 - acc: 0.8700 - val_loss: 0.5136 - val_acc: 0.7680 Epoch 15/30 100/100 [==============================] - 8s 83ms/step - loss: 0.2889 - acc: 0.8780 - val_loss: 0.5859 - val_acc: 0.7420 Epoch 16/30 100/100 [==============================] - 8s 83ms/step - loss: 0.2630 - acc: 0.8945 - val_loss: 0.6064 - val_acc: 0.7280 Epoch 17/30 100/100 [==============================] - 8s 84ms/step - loss: 0.2426 - acc: 0.9040 - val_loss: 0.5460 - val_acc: 0.7680 Epoch 18/30 100/100 [==============================] - 8s 84ms/step - loss: 0.2319 - acc: 0.9060 - val_loss: 0.6261 - val_acc: 0.7240 Epoch 19/30 100/100 [==============================] - 8s 83ms/step - loss: 0.2078 - acc: 0.9220 - val_loss: 0.5602 - val_acc: 0.7560 Epoch 20/30 100/100 [==============================] - 8s 84ms/step - loss: 0.1869 - acc: 0.9265 - val_loss: 0.5642 - val_acc: 0.7680 Epoch 21/30 100/100 [==============================] - 9s 86ms/step - loss: 0.1781 - acc: 0.9370 - val_loss: 0.5850 - val_acc: 0.7520 Epoch 22/30 100/100 [==============================] - 8s 84ms/step - loss: 0.1512 - acc: 0.9430 - val_loss: 0.6242 - val_acc: 0.7620 Epoch 23/30 100/100 [==============================] - 9s 85ms/step - loss: 0.1390 - acc: 0.9550 - val_loss: 0.6391 - val_acc: 0.7610 Epoch 24/30 100/100 [==============================] - 9s 87ms/step - loss: 0.1243 - acc: 0.9555 - val_loss: 0.6153 - val_acc: 0.7600 Epoch 25/30 100/100 [==============================] - 8s 84ms/step - loss: 0.1054 - acc: 0.9695 - val_loss: 0.6605 - val_acc: 0.7610 Epoch 26/30 100/100 [==============================] - 8s 83ms/step - loss: 0.1005 - acc: 0.9700 - val_loss: 0.6694 - val_acc: 0.7580 Epoch 27/30 100/100 [==============================] - 8s 83ms/step - loss: 0.0829 - acc: 0.9740 - val_loss: 0.6818 - val_acc: 0.7600 Epoch 28/30 100/100 [==============================] - 8s 83ms/step - loss: 0.0732 - acc: 0.9770 - val_loss: 0.7170 - val_acc: 0.7580 Epoch 29/30 100/100 [==============================] - 8s 83ms/step - loss: 0.0607 - acc: 0.9845 - val_loss: 0.7640 - val_acc: 0.7580 Epoch 30/30 100/100 [==============================] - 8s 83ms/step - loss: 0.0531 - acc: 0.9870 - val_loss: 0.9531 - val_acc: 0.7300 . model.save(&#39;/content/drive/My Drive/CNNs/cats_and_dogs_small_1.h5&#39;) . tf.keras.models.load_model(&#39;/content/drive/My Drive/CNNs/cats_and_dogs_small_1.h5&#39;, compile=False) . Curvas de precisi&#243;n y p&#233;rdida . import matplotlib.pyplot as plt acc = history.history[&#39;acc&#39;] val_acc = history.history[&#39;val_acc&#39;] max_val_acc = max(val_acc) max_val_acc_epoch= val_acc.index(max(val_acc)) + 1 loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] min_val_loss = min(val_loss) min_val_loss_epoch= val_loss.index(min(val_loss)) + 1 epochs = range(len(acc)) plt.plot(epochs, acc, &#39;go&#39;, label=&#39;Precisión en el entrenamiento&#39;) plt.plot(epochs, val_acc, &#39;r&#39;, label=&#39;Precisión en la validación&#39;) plt.plot(max_val_acc_epoch, max_val_acc, &#39;bo&#39;, label=&#39;Maximum accuracy&#39;) plt.title(&#39;Precisión durante el entrenamiento y la validación&#39;) plt.legend() print(&#39;Best accuracy epoch &#39;, max_val_acc_epoch) plt.figure() plt.plot(epochs, loss, &#39;go&#39;, label=&#39;Pérdida en el entrenamiento&#39;) plt.plot(epochs, val_loss, &#39;r&#39;, label=&#39;Pérdida en la validación&#39;) plt.plot(min_val_loss_epoch, min_val_loss, &#39;bo&#39;, label=&#39;Minimum loss&#39;) plt.title(&#39;Pérdida durante el entrenamiento y la validación&#39;) plt.legend() print(&#39;Best loss epoch &#39;, min_val_loss_epoch) plt.show() . Best accuracy epoch 14 Best loss epoch 14 . Aumentaci&#243;n de datos . Mediante el proceso de aumentación de datos, Keras brinda la posibilidad de incrementar el número de imágenes del conjunto de entrenamiento, a través de la generación de imágenes sintéticas. Algunas de las transformaciones para aumentación de datos, son las siguientes: rotation_range = Orientación aleatoria de las imágenes en el rango de 0-180. width_shift_range/height_shift_range = Traslación horizontal/vertical de las imágenes en un rango aleatorio determinado. Este rango está delimitado por las dimensiones de la imagen. shear_range = Ángulo aleatorio de inclinación de la imagen. A diferencia de la rotación, en shear_range se deja fijo un eje y a partir de este, se realiza la inclinación. Visualmente, esta transformación da la impresión de estirar la imagen. zoom_range = Zoom aleatorio a la imagen. horizontal_flip = Cuando su valor es de True, la mitad de las imágenes se voltean horizontalmente. Las imágenes que se voltean se eligen de forma aleatoria. fill_mode = Estrategia para rellenar las zonas de la imagen que quedan vacías al aplicar transformaciones como rotation_range o width_shift_range/height_shift_range. . datagen = ImageDataGenerator( rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=&#39;nearest&#39;) . from keras.preprocessing import image fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)] # Elegimos una imagen para aumentar img_path = fnames[3] # Leemos y redimensionamos la imagen img = image.load_img(img_path, target_size=(150, 150)) # Convertimos la imagen a un numpy array con dimensiones (150, 150, 3) x = image.img_to_array(img) # Redimensionamos el array a (1, 150, 150, 3) x = x.reshape((1,) + x.shape) # El comando flow () genera lotes de imágenes transformadas de forma aleatoria. # Este flujo es infinito, así que tenemos que terminarlo explícitamente. i = 0 for batch in datagen.flow(x, batch_size=1): plt.figure(i) imgplot = plt.imshow(image.array_to_img(batch[0])) i += 1 if i % 4 == 0: break plt.show() . datagen = ImageDataGenerator( #rotation_range=40, #width_shift_range=0.2, #height_shift_range=0.2, #shear_range=0.7) zoom_range=0.8) #horizontal_flip=True #fill_mode=&#39;nearest&#39;) . from keras.preprocessing import image fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)] # Elegimos una imagen para aumentar img_path = fnames[3] # Leemos y redimensionamos la imagen img = image.load_img(img_path, target_size=(150, 150)) # Convertimos la imagen a un numpy array con dimensiones (150, 150, 3) x = image.img_to_array(img) # Redimensionamos el array a (1, 150, 150, 3) x = x.reshape((1,) + x.shape) # El comando flow () genera lotes de imágenes transformadas de forma aleatoria. # Este flujo es infinito, así que tenemos que terminarlo explícitamente. i = 0 for batch in datagen.flow(x, batch_size=1): plt.figure(i) imgplot = plt.imshow(image.array_to_img(batch[0])) i += 1 if i % 4 == 0: break plt.show() . Definimos de nuevo la arquitectura, pero ahora considerando que usaremos aumentaci&#243;n de datos. . from keras import layers from keras import models from keras import optimizers model = models.Sequential() model.add(layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(150, 150, 3))) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;)) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(128, (3, 3), activation=&#39;relu&#39;)) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(128, (3, 3), activation=&#39;relu&#39;)) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Flatten()) model.add(layers.Dropout(0.5)) model.add(layers.Dense(512, activation=&#39;relu&#39;)) model.add(layers.Dense(1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=optimizers.RMSprop(lr=1e-4), metrics=[&#39;acc&#39;]) . Preprocesamiento de las im&#225;genes de entrada . train_datagen = ImageDataGenerator( rescale=1./255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True,) # El conjunto de validación no debe ser aumentado test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_directory( # Carpeta del conjunto de entrenamiento train_dir, # Las imágenes se redimensionan a 150 X 150 target_size=(150, 150), batch_size=20, #Al emplear binary_crossentropy como función de pérdida, las etiquetas deben ser binarias. class_mode=&#39;binary&#39;) validation_generator = test_datagen.flow_from_directory( validation_dir, target_size=(150, 150), batch_size=20, class_mode=&#39;binary&#39;) . Found 2000 images belonging to 2 classes. Found 1000 images belonging to 2 classes. . Entrenamiento . history = model.fit( train_generator, steps_per_epoch=100, epochs=100, validation_data=validation_generator, validation_steps=50) . model.save(&#39;/content/drive/My Drive/CNNs/cats_and_dogs_small_2.h5&#39;) . Curvas de precisi&#243;n y p&#233;rdida . import matplotlib.pyplot as plt acc = history.history[&#39;acc&#39;] val_acc = history.history[&#39;val_acc&#39;] max_val_acc = max(val_acc) max_val_acc_epoch= val_acc.index(max(val_acc)) + 1 loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] min_val_loss = min(val_loss) min_val_loss_epoch= val_loss.index(min(val_loss)) + 1 epochs = range(len(acc)) plt.plot(epochs, acc, &#39;go&#39;, label=&#39;Precisión en el entrenamiento&#39;) plt.plot(epochs, val_acc, &#39;r&#39;, label=&#39;Precisión en la validación&#39;) plt.plot(max_val_acc_epoch, max_val_acc, &#39;bo&#39;, label=&#39;Maximum accuracy&#39;) plt.title(&#39;Precisión durante el entrenamiento y la validación&#39;) plt.legend() print(&#39;Best accuracy epoch &#39;, max_val_acc_epoch) plt.figure() plt.plot(epochs, loss, &#39;go&#39;, label=&#39;Pérdida en el entrenamiento&#39;) plt.plot(epochs, val_loss, &#39;r&#39;, label=&#39;Pérdida en la validación&#39;) plt.plot(min_val_loss_epoch, min_val_loss, &#39;bo&#39;, label=&#39;Minimum loss&#39;) plt.title(&#39;Pérdida durante el entrenamiento y la validación&#39;) plt.legend() print(&#39;Best loss epoch &#39;, min_val_loss_epoch) plt.show() . Transferencia de aprendizaje / Fine-tuning . En este caso, utilizaremos la arquitectura VGG16, sin embargo, la forma de importar las otras arquitecturas es exactamente la misma. Para más información sobre la arquitectura VGG16, por favor revisar la siguiente referencia: Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition,” arXiv (2014), https://arxiv.org/abs/1409.1556. . from keras.applications import VGG16 conv_base = VGG16(weights=&#39;imagenet&#39;, include_top=False, input_shape=(150, 150, 3)) . conv_base.summary() . for layer in conv_base.layers: if layer.name[:6] == &#39;block5&#39;: layer.trainable = True else: layer.trainable = False . conv_base.summary() . Model: &#34;vgg16&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 150, 150, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 150, 150, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 150, 150, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 75, 75, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 75, 75, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 75, 75, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 37, 37, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 37, 37, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 37, 37, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 37, 37, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 18, 18, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 18, 18, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 18, 18, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 18, 18, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 9, 9, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 9, 9, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 9, 9, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 9, 9, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 4, 4, 512) 0 ================================================================= Total params: 14,714,688 Trainable params: 7,079,424 Non-trainable params: 7,635,264 _________________________________________________________________ . Definimos la arquitectura basada en transfer learning . model = models.Sequential() model.add(conv_base) model.add(layers.Flatten()) model.add(layers.Dense(256, activation=&#39;relu&#39;)) model.add(layers.Dense(1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=optimizers.RMSprop(lr=1e-4), metrics=[&#39;acc&#39;]) . Preprocesamiento de las im&#225;genes de entrada . train_datagen = ImageDataGenerator( rescale=1./255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True,) # El conjunto de validación no debe ser aumentado test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_directory( # Carpeta del conjunto de entrenamiento train_dir, # Las imágenes se redimensionan a 150 X 150 target_size=(150, 150), batch_size=20, #Al emplear binary_crossentropy como función de pérdida, las etiquetas deben ser binarias. class_mode=&#39;binary&#39;) validation_generator = test_datagen.flow_from_directory( validation_dir, target_size=(150, 150), batch_size=20, class_mode=&#39;binary&#39;) . Found 2000 images belonging to 2 classes. Found 1000 images belonging to 2 classes. . Entrenamiento . history = model.fit( train_generator, steps_per_epoch=100, epochs=100, validation_data=validation_generator, validation_steps=50) . model.save(&#39;/content/drive/My Drive/CNNs/cats_and_dogs_small_3.h5&#39;) . Curvas de precisi&#243;n y p&#233;rdida . import matplotlib.pyplot as plt acc = history.history[&#39;acc&#39;] val_acc = history.history[&#39;val_acc&#39;] max_val_acc = max(val_acc) max_val_acc_epoch= val_acc.index(max(val_acc)) + 1 loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] min_val_loss = min(val_loss) min_val_loss_epoch= val_loss.index(min(val_loss)) + 1 epochs = range(len(acc)) plt.plot(epochs, acc, &#39;go&#39;, label=&#39;Precisión en el entrenamiento&#39;) plt.plot(epochs, val_acc, &#39;r&#39;, label=&#39;Precisión en la validación&#39;) plt.plot(max_val_acc_epoch, max_val_acc, &#39;bo&#39;, label=&#39;Maximum accuracy&#39;) plt.title(&#39;Precisión durante el entrenamiento y la validación&#39;) plt.legend() print(&#39;Best accuracy epoch &#39;, max_val_acc_epoch) plt.figure() plt.plot(epochs, loss, &#39;go&#39;, label=&#39;Pérdida en el entrenamiento&#39;) plt.plot(epochs, val_loss, &#39;r&#39;, label=&#39;Pérdida en la validación&#39;) plt.plot(min_val_loss_epoch, min_val_loss, &#39;bo&#39;, label=&#39;Minimum loss&#39;) plt.title(&#39;Pérdida durante el entrenamiento y la validación&#39;) plt.legend() print(&#39;Best loss epoch &#39;, min_val_loss_epoch) plt.show() . Visualizaci&#243;n de activaci&#243;n intermedias . En las siguientes celdas, se definará el código que permitirá visualizar el resultado de las salidas de las capas convolucionales y max-pooling de la segunda CNN que entrenamos (en la que aplicamos aumentación de datos). . from keras.models import load_model #Cargamos el modelo que ya debe estar guardado en nuestro Google Drive model = load_model(&#39;/content/drive/My Drive/CNNs/cats_and_dogs_small_2.h5&#39;) model.summary() # As a reminder. . img_path = &#39;/content/cats_and_dogs_filtered/validation/dogs/dog.2104.jpg&#39; # Preprocesamos la imagen en un tensor 4D from keras.preprocessing import image import numpy as np img = image.load_img(img_path, target_size=(150, 150)) img_tensor = image.img_to_array(img) img_tensor = np.expand_dims(img_tensor, axis=0) #Recuerda que el modelo fue entrenado con imágenes #de entrada que se preprocesaron de la siguiente manera: img_tensor /= 255. # Su dimensión es de (1, 150, 150, 3) print(img_tensor.shape) . (1, 150, 150, 3) . import matplotlib.pyplot as plt plt.imshow(img_tensor[0]) plt.show() . from keras import models # Extracción de las salidas de las primeras 8 capas: layer_outputs = [layer.output for layer in model.layers[:8]] # Creación de un modelo que regresará estas salidas, dada una entrada: activation_model = models.Model(inputs=model.input, outputs=layer_outputs) . activations = activation_model.predict(img_tensor) . first_layer_activation = activations[7] print(first_layer_activation.shape) . (1, 7, 7, 128) . import matplotlib.pyplot as plt #Visualizamos la salida representada en el cuarto mapa de características de la primera capa convolucional plt.matshow(first_layer_activation[0, :, :, 127], cmap=&#39;viridis&#39;) plt.show() . plt.matshow(first_layer_activation[0, :, :, 30], cmap=&#39;viridis&#39;) plt.show() . import keras # Extraemos los nombres de las capas para mostrarlos en nuestra gráfica layer_names = [] for layer in model.layers[:8]: layer_names.append(layer.name) images_per_row = 16 # Visualicemos los mapas de características for layer_name, layer_activation in zip(layer_names, activations): # Este es el número de neuronas que componen el mapa de características en cuestión n_features = layer_activation.shape[-1] # Las dimensiones del mapa de características son (1, size, size, n_features) size = layer_activation.shape[1] # Concatenaremos los canales de activación (mapas de características) en esta matriz n_cols = n_features // images_per_row display_grid = np.zeros((size * n_cols, images_per_row * size)) # Procedemos a concatenar los canales de activación dentro de un marco o rejilla for col in range(n_cols): for row in range(images_per_row): channel_image = layer_activation[0, :, :, col * images_per_row + row] # Aplicamos un post-procesamiento a los mapas de características para que sean # visualmente ilustrativos channel_image -= channel_image.mean() channel_image /= channel_image.std() channel_image *= 64 channel_image += 128 channel_image = np.clip(channel_image, 0, 255).astype(&#39;uint8&#39;) display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image # Desplegamos la rejilla de canales de activación scale = 1. / size plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0])) plt.title(layer_name) plt.grid(False) plt.imshow(display_grid, aspect=&#39;auto&#39;, cmap=&#39;viridis&#39;) plt.show() . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide . Visualizaci&#243;n de filtros convolucionales . En esta sección, visualizaremos cuáles son los filtros receptivos a una imagen de entrada. Para llevar a cabo este procesamiento, usaremos el método de ascenso de gradiente, el cual será aplicado a una imagen de entrada a escala de grises, donde los valores de los píxeles se definirán aleatoriamente. Mediante este método, los píxeles de la imagen a escala de grises se ajustarán gradualmente hasta que un filtro en cuestión responda al máximo a la imagen. . Para implementar exitosamente este procedimiento, debemos construir una función de pérdida que maximice el valor de un filtro en específico, entonces, a través del ascenso de grandiente, se ajustarán los valores de lo píxeles de la imagen de entrada de tal forma que estos valores maximicen la activación del filtro. Para ilustrar lo anterior, a continuación definiremos una función de pérdida para la activación del filtro 0 de la capa &quot;block3_conv1&quot; de la arquitectura VGG16: . from keras.applications import VGG16 from keras import backend as K model = VGG16(weights=&#39;imagenet&#39;, include_top=False) layer_name = &#39;block3_conv1&#39; filter_index = 0 layer_output = model.get_layer(layer_name).output loss = K.mean(layer_output[:, :, :, filter_index]) . Para implementar ascenso de gradiente, usaremos la función gradients que viene incluida en el módulo backend de Keras: . import tensorflow as tf tf.compat.v1.disable_eager_execution() # La función gradiente retorna una lista de tensores, en este caso, la lista sólo contiene un elemento, # por eso es que tenemos que extraer ese único elemento. grads = K.gradients(loss, model.input)[0] . Con la finalidad de suavizar las actualizaciones realizadas durante el ascenso de gradiente, vamos a normalizar el gradiente extraído en la celda anterior. Esta normalización se llevará a cabo mediante la norma L2, que es la raíz cuadrada del promedio del cuadrado de los valores presentes en el tensor. Esta normalización, limita la magnitud de las actualizaciones hechas a los píxeles de la imagen de entrada a un mismo rango. . grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5) . Ahora que ya tenemos los prototipos de las funciones que permiten obtener la pérdida y el gradiente, tenemos que integrarlas en otra función que sea resposable de calcular tanto la pérdida como el gradiente dada una imagen de entrada. Para ello, usaremos un función backend de Keras: iterate. Esta función, recibe como entrada a la imagen de entrada y a las funciones de pérdida y de ascenso de gradiente, generando como resultado, los valores de la pérdida y del gradiente. . iterate = K.function([model.input], [loss, grads]) # Llamada a la función iterate import numpy as np loss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))]) . Ahora, definiremos un loop para aplicar sucesivamente el ascenso de gradiente: . input_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128. # El ascenso de gradiente se aplicará durante 40 épocas: lr = 1. # tasa de aprendizaje for i in range(40): # Calculo de las valores de pérdida y de gradiente loss_value, grads_value = iterate([input_img_data]) # Actualizamos los píxeles de la imagen de entrada en una dirección que maximice la pérdida input_img_data += grads_value * lr . La imagen resultante, tendrá dimensiones (1, 150, 150, 3). Además, los valores de cada píxel serán de tipo float y es posible que no estén en el rango de [0, 255], así que para compensar esto, aplicaremos la siguiente función de post-procesamiento: . def deprocess_image(x): # Normalización del tensor: centrado en 0 y con desviación estándar de 0.1 x -= x.mean() x /= (x.std() + 1e-5) x *= 0.1 # Nos aseguramos que cada píxel tenga un valor de 0 o 1 mediante la función clip. x += 0.5 x = np.clip(x, 0, 1) # Convertimos el tensor en un arreglo RGB x *= 255 x = np.clip(x, 0, 255).astype(&#39;uint8&#39;) return x . Ahora, retomemos las operaciones que definimos previamente y construyamos una función que tome como entrada el nombre de una capa y el índice de un filtro, y que retorne una imagen que represente el filtro que se activa al máximo a partir de la imagen de entrada a escala de grises. . def generate_pattern(layer_name, filter_index, size=150): # Definimos una función de pérdida que maximice la activación # de un filtro determinado perteneciente a una capa determinada. layer_output = model.get_layer(layer_name).output loss = K.mean(layer_output[:, :, :, filter_index]) # Definimos la función ascenso de gradiente grads = K.gradients(loss, model.input)[0] # Normalización del gradiente grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5) # La función iterate retorna la pérdida y el gradiente de acuerdo a una imagen de entrada iterate = K.function([model.input], [loss, grads]) # Nuestra imagen de entrada es una imagen a escala de grises con algo de ruido input_img_data = np.random.random((1, size, size, 3)) * 20 + 128. # El ascenso de gradiente se aplicará durante 40 épocas: step = 1. for i in range(40): loss_value, grads_value = iterate([input_img_data]) input_img_data += grads_value * step img = input_img_data[0] return deprocess_image(img) . plt.imshow(generate_pattern(&#39;block5_conv3&#39;, 12)) plt.show() . Buscando tener un panorma más amplio de los filtros que responden a la imagen de entrada, a continuación concatenaremos múltiples filtros en una rejilla. Específicamente, mostraremos los primeros 64 filtros de la primera capa convolucional de cada bloque de la arquitectura VGG16. La rejilla será de 8X8, y entre cada filtro, colocaremos un margen negro. . conv_base.summary() . Model: &#34;vgg16&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 150, 150, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 150, 150, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 150, 150, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 75, 75, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 75, 75, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 75, 75, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 37, 37, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 37, 37, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 37, 37, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 37, 37, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 18, 18, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 18, 18, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 18, 18, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 18, 18, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 9, 9, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 9, 9, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 9, 9, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 9, 9, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 4, 4, 512) 0 ================================================================= Total params: 14,714,688 Trainable params: 7,079,424 Non-trainable params: 7,635,264 _________________________________________________________________ . for layer_name in [&#39;block1_conv1&#39;, &#39;block2_conv1&#39;, &#39;block3_conv1&#39;, &#39;block4_conv1&#39;]: size = 64 margin = 5 # Definimos una imagen negra que será el marco de la rejilla y nos servirá para almacenar cada filtro. results = np.zeros((8 * size + 7 * margin, 8 * size + 7 * margin, 3)) for i in range(8): # iteramos sobre las filas de la rejilla for j in range(8): # iteramos sobre las columnas de la rejilla # Obtenemos el filtro activado número &#39;i + (j * 8)&#39; ubicado en la capa &#39;layer_name&#39; filter_img = generate_pattern(layer_name, i + (j * 8), size=size) # Ponemos el resultado en el cuadrado &#39;(i, j)&#39; correspondiente a una posición de la rejilla horizontal_start = i * size + i * margin horizontal_end = horizontal_start + size vertical_start = j * size + j * margin vertical_end = vertical_start + size results[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img # Desplegamos la rejilla plt.figure(figsize=(20, 20)) plt.imshow(results) plt.show() . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Visualizaci&#243;n de mapas de calor de clases de activaci&#243;n (heatmaps of class activation) . Mediante esta técnica podemos identificar de forma visual, cuáles son las regiones de la imagen de entrada que la red neuronal considera esenciales para realizar la clasificación. Aplicando esta técnica, se puede &quot;debuggear&quot; el funcionamiento de una red neuronal, sobre todo, cuando se tiene un error en la clasificación. Además, también es posible ubicar objetos específicos en una imagen. Esta técnica forma parte de un conglomerado de técnicas llamado visualización &quot;Class Activation Map&quot; (CAM), y consiste en producir mapas de calor de &quot;clases de activación&quot; sobre imágenes de entrada. Un mapa de calor de &quot;clase de activación&quot; es un tensor 2D que indica que tan importante es cierta región de la imagen con respecto a una clase en particular. Por ejemplo, dada una imagen de entrada de un perro, CAM genera un mapa de calor para la clase perro, y este mapa de calor puede posicionarse en una zona de la imagen del perro que sea característica de este, como las orejas o el hocico. La implementación de CAM que se utilizará, consiste en tomar los mapas de características producidos por una capa convolucional a partir de una imagen de entrada, y aplicarle a cada mapa de características el gradiente de la clase con respecto a dicho mapa. De forma intuitiva, una forma de entender este procedimiento, es que estamos ponderando &quot;la intensidad con la que la imagen de entrada activa diferentes canales o mapas de activación&quot; con respecto a &quot;cuán importante es cada canal con respecto a la clase&quot;, resultando en &quot;la intensidad con la que la imagen de entrada activa la clase&quot;. La implementación que realizaremos, proviene de la siguiente fuente: Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra, “Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization,” arXiv (2016), https://arxiv.org/abs/1610.02391. . . from keras.applications.vgg16 import VGG16 K.clear_session() # Observen como están cargando la arquitectura completa, incluyendo la capa final de clasificación. model = VGG16(weights=&#39;imagenet&#39;) . model.summary() . Model: &#34;vgg16&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 25088) 0 _________________________________________________________________ fc1 (Dense) (None, 4096) 102764544 _________________________________________________________________ fc2 (Dense) (None, 4096) 16781312 _________________________________________________________________ predictions (Dense) (None, 1000) 4097000 ================================================================= Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 _________________________________________________________________ . from keras.preprocessing import image from keras.applications.vgg16 import preprocess_input, decode_predictions import numpy as np # Ruta donde se encuentra almacenada la imagen img_path = &#39;/content/drive/My Drive/CNNs/elephants.jpg&#39; # Cargamos la imagen con las dimensiones correctas img = image.load_img(img_path, target_size=(224, 224)) # &#39;x&#39; es un numpy array tipo float32 con dimensiones (224, 224, 3) x = image.img_to_array(img) # Agregamos una dimensión para transformar nuestro arreglo en un # batch con dimensiones (1, 224, 224, 3) x = np.expand_dims(x, axis=0) # Preprocesamos el batch. Este preprocesamiento # normaliza cada canal x = preprocess_input(x) . preds = model.predict(x) print(&#39;Predicted:&#39;, decode_predictions(preds, top=3)[0]) . Predicted: [(&#39;n02504458&#39;, &#39;African_elephant&#39;, 0.9094213), (&#39;n01871265&#39;, &#39;tusker&#39;, 0.08618258), (&#39;n02504013&#39;, &#39;Indian_elephant&#39;, 0.004354576)] . np.argmax(preds[0]) . 386 . african_elephant_output = model.output[:, 386] # Esta es la salida de la capa convolucional &#39;block5_conv3&#39;, # que es la última capa convolucional de la arquitectura VGG16 last_conv_layer = model.get_layer(&#39;block5_conv1&#39;) # Este es el gradiente de la clase &quot;elefante africano&quot; con respecto # a los mapas de características de la capa &#39;block5_conv3&#39; grads = K.gradients(african_elephant_output, last_conv_layer.output)[0] # Este es un vector de dimensiones (512, ), donde cada elemento # representa la intensidad media del gradiente sobre un mapa de características específico pooled_grads = K.mean(grads, axis=(0, 1, 2)) # Esta función permite acceder a los valores de las cantidades que acabamos de definir: # &#39;pooled grads&#39; y los mapas de características de la capa &#39;block5_conv3&#39;, dada una imagen de entrada iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]]) # Estos son los valores de estas dos cantidades, representadas como numpy arrays, de acuerdo # a la imagen de entrada de los dos elefantes pooled_grads_value, conv_layer_output_value = iterate([x]) # Multiplicamos cada mapa de características por &quot;cuan importante este mapa de características es&quot; # con respecto a la clase elefante. for i in range(512): conv_layer_output_value[:, :, i] *= pooled_grads_value[i] # Obtenemos la media de cada elemento de cada mapa de características, generando así # nuestro mapa de calor de la clase de activación heatmap = np.mean(conv_layer_output_value, axis=-1) . heatmap = np.maximum(heatmap, 0) heatmap /= np.max(heatmap) plt.matshow(heatmap) plt.show() . import cv2 # Utilizamos opencv para cargar la imagen original img = cv2.imread(img_path) # Redimensionamos el mapa de calor para que tenga el mismo tamaño que la imagen original heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) # Convertimos el mapa de calor a RGB heatmap = np.uint8(255 * heatmap) # Aplicamos el mapa de calor a la imagen original heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # 0.4 es un factor de intensidad del mapa de calor superimposed_img = heatmap * 0.4 + img # Guardamos la imagen cv2.imwrite(&#39;/content/drive/My Drive/CNNs/elephant_cam3.jpg&#39;, superimposed_img) . True . La base de datos usada en este notebook, se obtuvo de la siguiente fuente: https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part1.ipynb .",
            "url": "https://marco-alberto.github.io/marco-perez-projects/jupyter/2022/04/21/_04_18_cats_dogs.html",
            "relUrl": "/jupyter/2022/04/21/_04_18_cats_dogs.html",
            "date": " • Apr 21, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://marco-alberto.github.io/marco-perez-projects/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Aquí irá mi información de contacto. .",
          "url": "https://marco-alberto.github.io/marco-perez-projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://marco-alberto.github.io/marco-perez-projects/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}